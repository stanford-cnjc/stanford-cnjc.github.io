{
  "sessions": [
    {
      "date": "2018-12-06",
      "series": "",
      "speakers": [
        {
          "name": "Tucker Fisher",
          "handle": "tgfisher",
          "domain": "stanford.edu",
          "url": ""
        },
        {
          "name": "Eshed Margalit",
          "handle": "eshedm",
          "domain": "stanford.edu",
          "url": "http://www.eshedmargalit.com"
        }
      ],
      "title": "CNJC Welcome and Introduction",
      "location": "LKSC 308",
      "time": "5:00PM",
      "files": [
        {
          "url": "https://docs.google.com/presentation/d/1vknVYQqJathSat6PnTUZcEWOQQOxnDSFTPMcEvObSq0/edit?usp=sharing",
          "name": "Session Slide Deck"
        }
      ]
    },
    {
      "date": "2019-01-23",
      "series": "",
      "speakers": [
        {
          "name": "Eshed Margalit",
          "handle": "eshedm",
          "domain": "stanford.edu",
          "url": "http://www.eshedmargalit.com"
        },
        {
          "name": "Tucker Fisher",
          "handle": "tgfisher",
          "domain": "stanford.edu",
          "url": ""
        }
      ],
      "title": "Visualizing High-Dimensional Data with t-SNE",
      "description": "This meeting will explore and shed light on the t-SNE algorithm, an increasingly popular technique for visualization of high-dimensional data. We will discuss example cases from systems neuroscience, genetics and deep learning, while unpacking the algorithm and sharing practical tips to deploy t-SNE in your own work. Graduate students and postdocs are both encouraged to attend!",
      "location": "LKSC 209",
      "time": "6:00PM",
      "files": [
        {
          "url": "https://s3-us-west-1.amazonaws.com/stanford-cnjc-files/2018/jan/session_01/JMLR_2008.pdf",
          "name": "Visualizing High-Dimensional Data Using t-SNE (van der Maaten et al., 2008)"
        },
        {
          "url": "https://distill.pub/2016/misread-tsne/",
          "name": "How to Use t-SNE Effectively (distill.pub)"
        },
        {
          "url": "https://elifesciences.org/articles/34275",
          "name": "Optogenetic dissection of descending behavioral control in Drosophila (Cande et al. 2018)"
        },
        {
          "url": "https://s3-us-west-1.amazonaws.com/stanford-cnjc-files/2019/jan/session_01/Quadrato_et_al_2017.pdf",
          "name": "Cell diversity and network dynamics in photosensitive human brain organoids (Quadrato et al., 2017)"
        },
        {
          "url": "https://docs.google.com/presentation/d/1Cs82iV2hya07nIFiSSOJbL7yZ6UTfMsFdK9fpfu3Lwg/edit?usp=sharing",
          "name": "Session Slide Deck"
        },
        {
          "url": "https://github.com/stanford-cnjc/cnjc-code/tree/master/meetings/2019_01_tSNE",
          "name": "Code used to generate figures"
        }
      ]
    },
    {
      "date": "2019-02-06",
      "series": "",
      "speakers": [
        {
          "name": "Blue Sheffer",
          "url": "http://www.bluesheffer.com/",
          "handle": "bluesheffer",
          "domain": "gmail.com"
        },
        {
          "name": "Guy Wilson",
          "url": "http://guyhwilson.com/",
          "handle": "ghwilson",
          "domain": "stanford.edu"
        }
      ],
      "title": "Linear Dynamical Systems for Time-Series Data Analysis",
      "description": "This meeting will focus on using linear dynamical systems (LDS) for analyzing high-dimensional time-series data in neuroscience. We will go over the math behind LDS, discuss examples of their use in neuroscience, and get hands-on experience fitting and visualizing LDS using Jupyter Notebooks. Graduate students and postdocs are both encouraged to attend!",
      "location": "LKSC 209",
      "time": "6:00PM",
      "rsvp_link": "https://docs.google.com/forms/d/e/1FAIpQLScWN8xcLJeQQMosPLT601Crg3bxAU9vySZGgfPR8yIhYOGz1Q/viewform?usp=sf_link",
      "files": [
        {
          "url": "https://www.tinyurl.com/linear4lyfe",
          "name": "Jupyter Notebook tutorial on linear dynamical systems"
        },
        {
          "url": "http://mlg.eng.cam.ac.uk/zoubin/papers/lds.pdf",
          "name": "A Unifying Review of Linear Gaussian Models"
        },
        {
          "url": "https://www.nature.com/articles/ncomms8759",
          "name": "Single-trial dynamics of motor cortex and their applications to brain-machine interfaces (Kao et al. 2015)"
        },
        {
          "url": "https://www.frontiersin.org/articles/10.3389/fncir.2014.00020/full",
          "name": "Dynamical criticality during induction of anesthesia in human ECoG recordings (Alonso et al. 2014)"
        },
        {
          "url": "https://github.com/slinderman/ssm",
          "name": "SSM: Bayesian learning and inference for state space models (Scott Linderman)"
        },
        {
          "url": "https://docs.google.com/presentation/d/1V6uyyHf6FUIuQQsv641_rPI9qkmGrNSgaUW10sQCjZM/edit?usp=sharing",
          "name": "Session Slide Deck"
        },
        {
          "url": "https://s3-us-west-1.amazonaws.com/stanford-cnjc-files/2019/feb/session_01/N2+on+food+L_2010_04_08__11_25_23___8___1_featuresN.hdf5",
          "name": "Raw worm movement data (30.6 MB)"
        }
      ]
    },
    {
      "date": "2019-02-20",
      "series": "",
      "speakers": [
        {
          "name": "Arielle Keller",
          "url": "https://arielleskeller.wixsite.com/attention",
          "handle": "askeller",
          "domain": "stanford.edu"
        },
        {
          "name": "Anna Khazenzon",
          "url": "",
          "handle": "annakhaz",
          "domain": "stanford.edu"
        }
      ],
      "title": "Critical Evaluation of Dynamic Functional Connectivity and other Methods for Time-Series Analysis",
      "description": "We will be discussing the promises and pitfalls of techniques such as sliding-window dynamic functional connectivity to assess brain dynamics. We will also touch on other methods for analysis of time series data in the context of human neuroimaging and electroencephalography. Graduate students and postdocs are both encouraged to attend!",
      "location": "LKSC 209",
      "time": "6:00PM",
      "rsvp_link": "https://docs.google.com/forms/d/e/1FAIpQLSdNGGLM9lTIHuZZWkzvU0E8JifUrM3mz_ZxCk-9Oetm75fA1w/viewform?usp=sf_link",
      "files": [
        {
          "url": "https://docs.google.com/presentation/d/1SA4-AO0tzT4e8LebgYN9QnBkJCPSW5TVVwqXEk4ppmo/edit?usp=sharing",
          "name": "Session Slide Deck"
        }
      ]
    },
    {
      "date": "2019-03-20",
      "series": "",
      "speakers": [
        {
          "name": "",
          "url": ""
        }
      ],
      "title": "Speed Friending",
      "description": "Get to know your fellow CNJCitizens at this casual speed-friending event. Discuss science, argue over the best coffee shop on campus, and meet your future presentation partner in this fun format!",
      "location": "LKSC 203/204",
      "time": "6:00PM",
      "files": []
    },
    {
      "date": "2019-04-03",
      "series": "",
      "speakers": [
        {
          "name": "Max Kanwal",
          "url": "",
          "handle": "kanwal",
          "domain": "stanford.edu"
        },
        {
          "name": "Saarthak Sarup",
          "url": "",
          "handle": "ssarup",
          "domain": "stanford.edu"
        }
      ],
      "title": "Predictive Coding",
      "location": "LKSC 208",
      "time": "6:00PM",
      "rsvp_link": "https://forms.gle/3Dudb1q88V5AvZK58",
      "files": [
        {
          "url": "https://docs.google.com/presentation/d/185M5JKIBP9X_kkQCfH7Oh0PKRzk6lMja_YTxR44yEjo/edit?usp=sharing",
          "name": "Session Slide Deck"
        },
        {
          "url": "https://github.com/saarthaks/PredictiveCodingESF",
          "name": "Github Repository"
        },
        {
          "url": "https://mybinder.org/v2/gh/saarthaks/PredictiveCodingESF/master",
          "name": "Jupyter Notebook Tutorial"
        },
        {
          "url": "http://machenslab.org/wp-content/uploads/2017/11/Boerlin_etal_2013.pdf",
          "name": "Predictive coding of dynamical variables in balanced spiking networks (Boerlin, Machens, and Denéve - 2013)"
        },
        {
          "url": "https://arxiv.org/pdf/1703.03777.pdf",
          "name": "Learning to represent signals spike by spike (Brendel et al. - 2017)"
        },
        {
          "url": "https://arxiv.org/pdf/1705.08026.pdf",
          "name": "Learning arbitrary dynamics in efficient, balanced spiking networks using local plasticity rules (Alemi and Machens - 2017)"
        }
      ]
    },
    {
      "date": "2019-04-17",
      "series": "",
      "speakers": [
        {
          "name": "Aran Nayebi",
          "url": "https://sites.google.com/site/anayebihomepage/",
          "handle": "anayebi",
          "domain": "stanford.edu"
        },
        {
          "name": "Josh Melander",
          "url": "https://github.com/jbmelander",
          "handle": "melander",
          "domain": "stanford.edu"
        }
      ],
      "title": "Deep networks and the brain: simile or metaphor?",
      "location": "LKSC 208",
      "time": "6:00PM",
      "description": "Deep neural networks have emerged as a powerful and flexible class of models for brain activity and behavior. But what is the objective of these modeling efforts? Do we intend to reproduce the brain in a 1-to-1 manner, or could more abstract models serve a purpose? This session will spark discussion of the history of brain modeling, current practices, and implications for future research.",
      "files": [
        {
          "url": "https://s3-us-west-1.amazonaws.com/stanford-cnjc-files/2019/apr/session_02/Chichilnisky_2001.pdf",
          "name": "A simple white noise analysis of neuronal light responses (Chichilnisky 2001)"
        },
        {
          "url": "https://s3-us-west-1.amazonaws.com/stanford-cnjc-files/2019/apr/session_02/yamins_dicarlo_2016.pdf",
          "name": "Using goal-driven deep learning models to understand sensory cortex (Yamins and DiCarlo, 2016)"
        },
        {
          "url": "https://s3-us-west-1.amazonaws.com/stanford-cnjc-files/2019/apr/session_02/simile_or_metaphor_slides.pdf",
          "name": "Session Slide Deck"
        }
      ]
    },
    {
      "date": "2019-05-01",
      "series": "",
      "speakers": [
        {
          "name": "Isabel Low",
          "url": "",
          "handle": "ilow",
          "domain": "stanford.edu"
        },
        {
          "name": "Arielle Keller",
          "url": "https://arielleskeller.wixsite.com/attention",
          "handle": "askeller",
          "domain": "stanford.edu"
        }
      ],
      "title": "NeuWrite/CNJC Crossover Event",
      "description": "Communicating science is never easy, but it can be especially challenging when your project falls in the gray space between scientific fields. Differences in buzz-words, techniques, and even long-term goals of the field can make it difficult to apply for fellowships or share your work with colleagues. We will discuss best practices for communicating interdisciplinary research and how to turn these potential pitfalls into strengths, in collaboration with the science communication group NeuWrite West.",
      "location": "LKSC 208",
      "time": "6:00PM",
      "rsvp_link": "https://forms.gle/XyevyHjF1tA37GGLA",
      "files": [
        {
          "url": "https://www.neuwritewest.org",
          "name": "NeuWrite West Webpage"
        },
        {
          "url": "https://docs.google.com/presentation/d/1QFcyF3h36DGNYHwqswDIfIt18HIRJVhst9YFasQmEcU/edit?usp=sharing",
          "name": "Session Slide Deck"
        }
      ]
    },
    {
      "date": "2019-05-22",
      "series": "",
      "speakers": [
        {
          "name": "MBCT Program",
          "url": ""
        }
      ],
      "title": "CNJC hosts 'MBCT Happy Hour!'",
      "description": "Come and meet others in the MBCT community. Thanks to MBCT for providing all sorts of drinks and snacks!",
      "location": "LKSC Stairs",
      "time": "6:00PM",
      "files": []
    },
    {
      "date": "2019-06-12",
      "series": "",
      "speakers": [
        {
          "name": "Eshed Margalit",
          "handle": "eshedm",
          "domain": "stanford.edu",
          "url": "http://www.eshedmargalit.com"
        }
      ],
      "title": "Feature Visualization in Artificial and Biological Neural Networks",
      "location": "LKSC 209",
      "time": "6:00PM",
      "description": "The use of neural networks in neuroscience is often met with two criticisms: that neural networks are 'black boxes' that defy interpretation, and that they have not proven useful for studying their biological counterparts. In this session, we will discuss and play with gradient-based methods for synthesizing images that maximally drive neurons, both in artificial neural networks and in the primate brain.",
      "files": [
        {
          "url": "https://science.sciencemag.org/content/sci/364/6439/eaav9436.full.pdf?ijkey=iBRdlniG7iYuA&keytype=ref&siteid=sci",
          "name": "Neural population control via deep image synthesis (Bashivan et al., 2019)"
        },
        {
          "url": "https://www.cell.com/cell/pdf/S0092-8674(19)30391-5.pdf",
          "name": "Evolving Images for Visual Neurons Using a Deep Generative Network Reveals Coding Principles and Neuronal Preferences (Ponce et al., 2019)"
        },
        {
          "url": "https://distill.pub/2017/feature-visualization/",
          "name": "distill.pub Post: Feature Visualization"
        },
        {
          "url": "https://docs.google.com/presentation/d/1Uig912uS0PBdah-UWTXmwMMMfJLmA09ZnwkvpGN-z2Y/edit?usp=sharing",
          "name": "Session Slide Deck"
        },
        {
          "url": "https://github.com/stanford-cnjc/cnjc-code/tree/master/meetings/2019_06_imageopt",
          "name": "Link to Notebooks on GitHub"
        }
      ]
    },
    {
      "date": "2019-07-10",
      "series": "",
      "speakers": [
        {
          "name": "Tyler Benster",
          "url": "https://www.tylerbenster.com",
          "handle": "tbenst",
          "domain": "stanford.edu"
        },
        {
          "name": "Aaron Andalman",
          "url": ""
        }
      ],
      "title": "Variational Bayesian Methods in Neuroscience",
      "location": "LKSC 209",
      "time": "6:00PM",
      "description": "Neuroscience data is partially observed, only including certain regions at a particular scale, and statistically dependent, as a single neuron synapses onto thousands of other neurons. Since this violates the assumptions for commonly used statistical tools, we turn to Bayesian methods. As exact Bayesian inference is often computationally intractable, this talk will cover exciting recent results for approximating Bayesian inference as a deep learning optimization problem.",
      "files": [
        {
          "url": "https://github.com/stanford-cnjc/cnjc-code/tree/master/meetings/2019_07_variational_bayesian",
          "name": "Link to GitHub repository (notebook and slides)"
        },
        {
          "url": "https://github.com/tbenst/cnjc-vae/blob/30dfc59193c892d26bc1b6f947a27f84bae4ec7e/main.pdf",
          "name": "Direct link to slide deck"
        }
      ]
    },
    {
      "date": "2019-09-04",
      "series": "",
      "speakers": [
        {
          "name": "Eshed Margalit",
          "url": "http://www.eshedmargalit.com",
          "domain": "stanford.edu",
          "handle": "eshedm"
        },
        {
          "name": "Tucker Fisher",
          "url": "http://tgfisher.github.io",
          "domain": "stanford.edu",
          "handle": "tgfisher"
        }
      ],
      "title": "Welcome Back!",
      "description": "Please join us for a casual welcome back get-together to celebrate the start of the academic year. In addition to giving a recap of last year's events, we'll discuss plans for the coming year and brainstorm topics of interest for future sessions. Oh, and there'll be dinner provided!",
      "location": "LKSC 209",
      "time": "6:00PM",
      "files": [
        {
          "url": "https://docs.google.com/presentation/d/1SVOdBAVtqfT5h03sUljNKr-f-Ct551qIZEDIwN_24Uo/edit?usp=sharing",
          "name": "Session Slide Deck"
        },
        {
          "url": "https://bit.ly/2m09sIt",
          "name": "Sign-up Spreadsheet"
        }
      ]
    },
    {
      "date": "2019-10-02",
      "series": "",
      "speakers": [
        {
          "name": "John Kochalka",
          "url": "",
          "domain": "stanford.edu",
          "handle": "kochalka"
        },
        {
          "name": "YoungJu Jo",
          "url": ""
        }
      ],
      "title": "Chaos in Recurrent Neural Networks",
      "description": "The brain is spontaneously active and many behaviors rely on complex, time-varying patterns of neural activity. This session will focus on random recurrent networks, a powerful class of nonlinear dynamical systems models whose intrinsic chaotic activity can be leveraged towards diverse tasks such as telling time or producing motor outputs. We will elucidate the basic mathematical concepts underlying these networks and explore some fun examples from the literature with interactive demos.",
      "rsvp_link": "https://docs.google.com/forms/d/e/1FAIpQLSfRiXkBBd7doBePixJ0UfpjIVIRDXXOhNRBJqzPQyNj3JSRRg/viewform?usp=sf_link",
      "location": "LKSC 209",
      "time": "6:00PM",
      "files": [
        {
          "url": "https://docs.google.com/presentation/d/1XzOjFi2bikPjnbTZRPjMuH0ex42KQiucogD6XmJsDo0/edit#slide=id.p",
          "name": "Session Slide Deck"
        },
        {
          "url": "https://colab.research.google.com/drive/1BEEZgksOl7hc7TGIrWq3GB_4orV6zee6?usp=drive_open",
          "name": "Colab Notebook: Visualizations (Static)"
        },
        {
          "url": "https://colab.research.google.com/drive/1XZPb9l598cV3s0q7Nl3GtOJCH_rnKsQf?usp=drive_open",
          "name": "Colab Notebook: Visualizations (Dynamic)"
        },
        {
          "url": "https://github.com/kochalka/CNJC_RNNs",
          "name": "Github Repo"
        },
        {
          "url": "https://github.com/ReScience-Archives/Vitay-2016",
          "name": "Python implementation of Laje and Buonomano, 2013"
        }
      ]
    },
    {
      "date": "2019-10-16",
      "series": "",
      "speakers": [
        {
          "name": "Tucker Fisher",
          "domain": "stanford.edu",
          "handle": "tgfisher",
          "url": ""
        }
      ],
      "title": "Journal Club: Computational Neuroethology: A Call to Action",
      "description": "We will be reading the following paper as a group. Please find the PDF attached and come prepared to discuss the contents. Paper abstract: \"The brain is worthy of study because it is in charge of behavior. A flurry of recent technical advances in measuring and quantifying naturalistic behaviors provide an important opportunity for advancing brain science. However, the problem of understanding unrestrained behavior in the context of neural recordings and manipulations remains unsolved, and developing approaches to addressing this challenge is critical. Here we discuss considerations in computational neuroethology—the science of quantifying naturalistic behaviors for understanding the brain—and propose strategies to evaluate progress. We point to open questions that require resolution and call upon the broader systems neuroscience community to further develop and leverage measures of naturalistic, unrestrained behavior, which will enable us to more effectively probe the richness and complexity of the brain.\"",
      "location": "LKSC 209",
      "time": "6:00PM",
      "files": [
        {
          "url": "https://stanford-cnjc-files.s3-us-west-1.amazonaws.com/2019/oct/session_02/Datta_2019.pdf",
          "name": "Computational Neuroethology: A Call to Action (Datta et al., 2019)"
        }
      ]
    },
    {
      "date": "2019-11-13",
      "series": "",
      "speakers": [
        {
          "name": "Gabriel Mel",
          "url": "",
          "domain": "stanford.edu",
          "handle": "meldefon"
        },
        {
          "name": "Ben Sorscher",
          "url": "",
          "domain": "gmail.com",
          "handle": "bsorsch"
        }
      ],
      "title": "Grid Cells in Recurrent Neural Networks",
      "description": "Recent studies have found that RNNs trained to solve a navigational task naturally develop periodic spatial tuning similar to that of medial entorhinal cortex (MEC) grid cells, raising the exciting possibility that such in silico models could give insight into MEC function or suggest novel recording and manipulation experiments. However, as of now almost nothing is known about the precise circuit mechanisms responsible for grid firing or path integration in these networks, posing a challenge both to neuroscientists and those interested in deep network analysis. Here we study the \"neurophysiology\" of these RNNs, and show that despite their apparent complexity and biologically characteristic \"messiness\", they can be understood in terms of a small number of intuitive network mechanisms. This analysis shows how grid firing arises within trained RNNs. Time permitting, we will present a mathematical theory that explains why hexagonal grids emerge as the optimal patterns for the navigation task.",
      "location": "LKSC 209",
      "time": "6:00PM",
      "files": [
        {
          "name": "Session Slide Deck",
          "url": "https://docs.google.com/presentation/d/1E4H1MxdJizO1wqrgIozycwUnLvh2tjGcP3xbT9Dei2E/edit?usp=sharing"
        }
      ]
    },
    {
      "date": "2019-12-11",
      "series": "",
      "speakers": [
        {
          "name": "Jay Bhasin",
          "url": ""
        },
        {
          "name": "Max Gagnon",
          "url": ""
        }
      ],
      "title": "The Eighty Five Percent Rule for optimal learning",
      "description": "A common principle in learning new knowledge or acquiring a new skill is to start with something easy, and progressively challenge yourself with the harder stuff. While this might make sense intuitively, in a recent paper, Wilson et al. show mathematically that for classifier models, learning by gradient descent proceeds at an optimal rate when the difficulty of the task is such that the classifier is approximately 85% accurate. In this week's CNJC we will go through the approach used in the paper and discuss this result.",
      "location": "LKSC 209",
      "time": "6:00PM",
      "files": [
        {
          "url": "https://www.nature.com/articles/s41467-019-12552-4",
          "name": "The Eighty Five Percent Rule for optimal learning. Wilson et al., 2019"
        }
      ]
    },
    {
      "date": "2020-01-08",
      "series": "",
      "speakers": [
        {
          "name": "Minseung Choi",
          "url": "https://minseung.people.stanford.edu/",
          "domain": "stanford.edu",
          "handle": "minseung"
        },
        {
          "name": "John Wen",
          "url": ""
        }
      ],
      "title": "HMM Smackdown: HMM-GLM vs. SLDS (HMM-LDS)",
      "description": "Hidden Markov Models (HMMs) have become a popular tool in neuroscience, especially when paired with other powerful techniques such as linear dynamical systems (LDS) models and generalized linear models (GLMs). In this session, two approaches (HMM-GLM and HMM-LDS) will be explained, compared and contrasted.",
      "location": "LKSC 209",
      "time": "6:00PM",
      "files": [
        {
          "url": "https://docs.google.com/presentation/d/1L4jdiE6c68mU73BnpbH44XhIAXTAcpgmIYF-U_ijv68/edit?usp=sharing",
          "name": "Session Slide Deck"
        },
        {
          "url": "https://stanford-cnjc-files.s3-us-west-1.amazonaws.com/2020/jan/session_01/CNJC.m",
          "name": "MATLAB Code from Session"
        },
        {
          "url": "https://stanford-cnjc-files.s3-us-west-1.amazonaws.com/2020/jan/session_01/Calhoun_et_al_2019.pdf",
          "name": "Unsupervised identification of the internal states that shape natural behavior (Calhoun et al., 2019)"
        },
        {
          "url": "https://stanford-cnjc-files.s3-us-west-1.amazonaws.com/2020/jan/session_01/Linderman_et_al_2017.pdf",
          "name": "Bayesian Learning and Inference in Recurrent Switching Linear Dynamical Systems (Linderman et al., 2017)"
        }
      ]
    },
    {
      "date": "2020-01-22",
      "series": "",
      "speakers": [
        {
          "name": "Mark Plitt",
          "url": "https://neuroscience.stanford.edu/people/mark-plitt"
        }
      ],
      "title": "Experience dependent contextual codes in the hippocampus",
      "description": "The hippocampus is a medial temporal lobe brain structure that contains circuitry and neural representations capable of supporting declarative memory. Hippocampal place cells fire in one or few restricted spatial locations in a given environment. Between environmental contexts, place cell firing fields remap (turning on/off or moving to a new spatial location), providing a unique population-wide neural code for context specificity. However, the manner by which features associated with a given context combine to drive place cell remapping remains a matter of debate. Here we show that remapping of neural representations in region CA1 of the hippocampus is strongly driven by prior beliefs about the frequency of certain contexts, and that remapping is equivalent to an optimal estimate of the identity of the current context under that prior. This prior-driven remapping is learned early in training and remains robust to changes in behavioral task-demands. Furthermore, a network model that uses a simple associative learning mechanism is sufficient to reproduce these results. Our findings demonstrate that place cell remapping is a generalization of representing an animal’s location. Rather than simply representing location in physical space, the hippocampus represents an optimal estimate of location in a multi-dimensional stimulus space.",
      "location": "LKSC 209",
      "time": "6:00PM",
      "files": [
        {
          "url": "https://drive.google.com/file/d/1av6NaY7l99w9swFvQ_eVKe2IfBf6XsOz/view?usp=sharing",
          "name": "Session Slide Deck"
        },
        {
          "url": "https://github.com/GiocomoLab/PlittGiocomo_CA1Morph_2019",
          "name": "Github Repository"
        }
      ]
    },
    {
      "date": "2020-02-05",
      "series": "",
      "speakers": [
        {
          "name": "Dawn Finzi",
          "url": ""
        },
        {
          "name": "Josh Ryu",
          "url": ""
        }
      ],
      "title": "Receptive fields: representations of feature and space in visual cortex",
      "description": "Receptive fields have been integral to the study of visual neuroscience since Hubel and Weisel. In this session we plan to discuss two approaches that leverage the concept of a 'receptive field' to probe representations in visual cortex: 1) population receptive models in fMRI and 2) CNNs with readout layers that factorize space and feature weights",
      "location": "LKSC 209",
      "time": "6:00PM",
      "rsvp_link": "https://forms.gle/JfEtQRBe2uoQZmvLA",
      "files": [
        {
          "url": "http://cvnlab.net/papers/KayCurrentBiology2015.pdf",
          "name": "Attention Reduces Spatial Uncertainty in Human Ventral Temporal Cortex (Kay et al., 2015)"
        },
        {
          "url": "http://bethgelab.org/media/publications/6942-neural-system-identification-for-large-populations-separating-what-and-where.pdf",
          "name": "Neural system identification for large populations separating 'what' and 'where' (Klindt et al., 2017)"
        },
        {
          "url": "https://github.com/dawnfinzi/pRFdemo",
          "name": "pRF Model Fitting Demo"
        }
      ]
    },
    {
      "date": "2020-02-19",
      "series": "",
      "speakers": [
        {
          "name": "Libby Zhang",
          "url": ""
        }
      ],
      "title": "Pose Estimation for Behavioral Analysis",
      "description": "Rich behavioral representations can aid in the investigation of neural dynamics and function. After an overview of a framework for behavioral analysis, I will review existing approaches in pose estimation. Pose estimation is a key analytical block for constructing these rich behavioral representations. I will discuss model-based and model-free approaches from human pose estimation literature and their applications in animal pose estimation. At the end, I will briefly propose a new approach that aims to strike a balance between the interpretability of model-based methods with the expressive power of model-free methods.",
      "rsvp_link": "https://forms.gle/y7cj6NANpTCcPyrW6",
      "location": "LKSC 209",
      "time": "6:00PM",
      "files": []
    },
    {
      "date": "2020-08-19",
      "series": "",
      "speakers": [
        {
          "name": "Anna Gillespie",
          "url": ""
        },
        {
          "name": "Eric Denovellis",
          "url": ""
        }
      ],
      "title": "Special Guest Session: Characterizing hippocampal replay using switching point process state space models",
      "description": "In the hippocampus, replay sequences are temporally compressed patterns of neural spiking that resemble patterns that occur when the animal is moving through the environment. Because replay sequences typically occur when the animal is at rest, replay is hypothesized to be part of an internal cognitive process that enables the retrieval of past spatial memories and the planning of future movement. Traditionally, replay sequences have been discovered by identifying sharp wave ripples (SWRs)—high frequency oscillations that occur in association with replay—and then looking within SWRs for spatially continuous patterns of neural spiking. This does not fully account for the content or timing of replay sequences, however. Replay sequences do not always co-occur with sharp wave ripples, have more complex dynamics than spatially continuous movement, have different temporal ordering than during movement, and may change based on task. <br /><br />In this talk, we will introduce a switching state space framework to describe the richness of replay sequences. We show how defining discrete latent states associated with continuous latent movement dynamics and point process observations allows us to identify when non-local replay sequences occur, categorize the type of sequence based on latent movement dynamics, and decode the spatial trajectory. We will then show how this can be applied to a complex operant conditioning task.",
      "location": "Zoom (Link and Password in Email)",
      "time": "6:00PM",
      "files": [
        {
          "url": "https://stanford.zoom.us/rec/share/xPIvN6_X-l9OHK_BtxraR4d9Aru-eaa8gXMarPQNy0puL0MtiYl7rKZKZAE8eEfh",
          "name": "Stream the Recorded Zoom Session"
        }
      ]
    },
    {
      "date": "2020-08-05",
      "series": "",
      "speakers": [
        {
          "name": "Daniel Bear",
          "url": "https://neuroscience.stanford.edu/people/daniel-bear",
          "handle": "dbear",
          "domain": "stanford.edu"
        }
      ],
      "title": "Learning Physical Graph Representations from Visual Scenes",
      "description": "Human vision is object-centric: we partition scenes into discrete objects and attribute physical properties to them, including appearance, position, shape, material composition, and their relations to each other. This abstraction is important for higher cognition, as most of our behaviors and goals are specified in terms of entities and relations &mdash; not pixel-level detail. Despite this, most state-of-the-art computer vision algorithms are based on image-like internal representations; if \"objects\" are present at all, they are typically given only \"semantic\" properties, not the physical ones needed for useful interaction. To begin addressing this gap, we've developed a way to augment computer vision algorithms with object-centric representations. We call these representations \"Physical Scene Graphs\" (PSGs) because they have both a hierarchical graphical structure and an explicit encoding of physical properties of a scene's visual elements, such as their position, shape, texture, and so on. Algorithms that infer PSG representations from visual input &mdash; \"PSGNets\" &mdash; groups visual features into discrete, unsupervised entities (\"objects\") and predict their physical attributes as explicit components of associated vectors. The algorithm is recursive, hierarchically constructing a graph of high-level objects from their low-level parts. I'll talk about the architecture of PSGNets, our results using them to segment visual scenes into objects without supervision, and &mdash; if there's time &mdash; how we're hoping to connect them back to neural circuits in biological visual systems.",
      "location": "Zoom (Link and Password in Email)",
      "time": "6:00PM",
      "files": [
        {
          "url": "https://arxiv.org/abs/2006.12373",
          "name": "arXiv Preprint"
        },
        {
          "url": "https://stanford.zoom.us/rec/share/3NV2N7bp9m5IeLf29kbUHfMALL66eaa82ndPrPIIxRkymsCpVmpdKADGagpSQBHT",
          "name": "Stream the Recorded Zoom Session"
        }
      ]
    },
    {
      "date": "2020-09-02",
      "series": "",
      "speakers": [
        {
          "name": "Scottie Alexander",
          "url": ""
        }
      ],
      "title": "Special Guest Session: Topic to be Announced",
      "description": "For decades, computational scientists have faced an inescapable trade off between performance and usability in programming languages. While high level languages (like Matlab and Python+NumPy) have made tremendous contributions by making high performance mathematical routines easily accessible, the computational sciences increasingly rely on novel, complex algorithms that cannot be efficiently expressed using existing optimized libraries, forcing users to often resort to \"performance languages\" like C when efficiency is paramount. The Julia programming language, first released in 2012, aims to solve this problem by creating a syntax that \"reads like python\", but code that \"runs like C\". In this talk I will introduce Julia, provide a semi-technical overview of how it achieves such high performance, and explain why the language is uniquely suited to meet the ever growing needs of computational scientists.",
      "location": "Zoom",
      "time": "6:00PM",
      "files": []
    },
    {
      "date": "2020-08-13",
      "series": "CNJCx",
      "speakers": [
        {
          "name": "Tucker Fisher",
          "handle": "tgfisher",
          "domain": "stanford.edu",
          "url": "https://tgfisher.github.io/#/"
        },
        {
          "name": "Eshed Margalit",
          "handle": "eshedm",
          "domain": "stanford.edu",
          "url": "http://www.eshedmargalit.com"
        },
        {
          "name": "Josh Melander",
          "handle": "melander",
          "domain": "stanford.edu",
          "url": ""
        }
      ],
      "title": "CNJCx: Practical Python Week 1",
      "description": "Having a comfortable, modern, and efficient development setup is the first step toward learning the tools and skills needed in the rest of the series. In this session, we'll introduce the CNJCx series, install common tools in computing and programming, discuss open source software, and go over basic navigation in the terminal. Attendees should be prepared to follow along at home (no experience required!) by making sure they can open a terminal (bash or zsh) on their computer ahead of time. If you need a few pointers to open the terminal on your operating system, please check out the <a href=\"https://stanford-cnjc.github.io/#/CNJCx\">CNJCx page</a> for instructions!",
      "location": "Zoom",
      "time": "5:00PM",
      "files": []
    },
    {
      "date": "2020-08-20",
      "series": "CNJCx",
      "speakers": [],
      "title": "CNJCx: Practical Python Week 2",
      "description": "The second session in the CNJCx series will cover working on a remote server: a common setup for many labs or individuals using services such as <a href=\"https://www.sherlock.stanford.edu/\">Sherlock, Stanford's High-Performance Computing Cluster</a>. We will also discuss tools that make working on remote machines more productive and painless, including Vim (a popular terminal-based text editor), and tmux: a tool to manage terminal sessions and keep them running even when you log out. These tools can also improve productivity on your local machine, i.e. your laptop or home computer. Other topics include file transfer, customizing your text editor, and SSH keys for secure remote login.",
      "location": "Zoom",
      "time": "5:00PM",
      "files": []
    },
    {
      "date": "2020-08-27",
      "series": "CNJCx",
      "speakers": [],
      "title": "CNJCx: Practical Python Week 3",
      "description": "In this session of CNJCx: Practical Python, we'll talk about the various ways to install and use Python, including a discussion of pip, conda, different versions of Python, virtual environments, and switching between Python versions. We'll also talk about popular alternatives to writing Python scripts, like Jupyter notebooks and Jupyter lab. Finally, we'll compare Python to MATLAB and describe some of the differences in how they're installed and used.",
      "location": "Zoom",
      "time": "5:00PM",
      "files": []
    },
    {
      "date": "2020-09-03",
      "series": "CNJCx",
      "speakers": [],
      "title": "CNJCx: Practical Python Week 4",
      "description": "Week 4 of the series focuses on project management at a higher level, a core skill that researchers need to develop. This includes writing clean code, version control with <code>git</code> using Github, synchronizing work across machines, formatting and linting code, and more!",
      "location": "Zoom",
      "time": "5:00PM",
      "files": []
    },
    {
      "date": "2020-09-10",
      "series": "CNJCx",
      "speakers": [],
      "title": "CNJCx: Practical Python Week 5",
      "description": "In week 5, we'll use the skills built up in previous sessions to look at many of the ways Python is being used for research in the biosciences. Topics include data analysis pipelines, figure-making, statistical testing, and using modern machine learning frameworks, e.g., scikit-learn and PyTorch.",
      "location": "Zoom",
      "time": "5:00PM",
      "files": []
    },
    {
      "date": "2020-09-17",
      "series": "CNJCx",
      "speakers": [
        {
          "name": "Toby Alden",
          "handle": "",
          "domain": "",
          "url": ""
        },
        {
          "name": "William Casarin",
          "handle": "",
          "domain": "",
          "url": ""
        }
      ],
      "title": "CNJCx: Practical Python Week 6",
      "description": "Week 6 will see two industry professionals share their perspective on project structure, code management, and best practices for scientific code. Following the guest lectures, we'll conclude the series by collecting feedback and sharing accumulated course materials.",
      "location": "Zoom",
      "time": "5:00PM",
      "files": []
    },
    {
      "date": "2018-06-26",
      "series": "",
      "speakers": [
        {
          "name": "Jay Bhasin",
          "url": ""
        }
      ],
      "title": "Systems Consolidation and Attractor Dynamics in Cerebellar Learning",
      "location": "LKSC 209",
      "time": "6:00PM",
      "files": []
    },
    {
      "date": "2018-05-23",
      "series": "",
      "speakers": [
        {
          "name": "Kevin Feigelis",
          "url": ""
        }
      ],
      "title": "Modular Continual Learning in a Unified Visual Environment",
      "location": "LKSC 209",
      "time": "6:00PM",
      "files": [
        {
          "url": "https://arxiv.org/abs/1711.07425",
          "name": "arXiv Link"
        }
      ]
    }
  ]
}
